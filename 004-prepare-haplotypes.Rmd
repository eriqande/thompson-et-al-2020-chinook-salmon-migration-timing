---
title: "Prepare Haplotypes in the GREB1L Region for Further Analyses"
author: "Eric C. Anderson"
date: "Last Updated: `r Sys.Date()`"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: true
bibliography: references.bib
---


The goal here is to infer haplotypes in the GREB1L region to be used
in later analyses.  We use a small portion of the VCF that is saved
in this repository in `data/greb1l-ish-region.vcf.gz`


```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
start_time <- Sys.time()
```

# Packages and paths 
```{r}
library(tidyverse)
library(vcfR)

dir.create("outputs/004", recursive = TRUE, showWarnings = FALSE)
dir.create("intermediates/004", recursive = TRUE, showWarnings = FALSE)
```



# Manipulating the VCF file

## Get sample names and prepare a file to reheader the vcf

```{r}
samples <- read_csv("data/wgs-chinook-samples.csv") %>%
  mutate(newname = paste0(Population, "-", NMFS_DNA_ID)) %>%
  mutate(newname = str_replace_all(newname, " +", "_"))

new_names <- samples %>%
  select(vcf_name, newname)

write_tsv(new_names, "intermediates/004/new_names_for_vcf.txt", col_names = FALSE)
```

## Use bcftools to reheader the file with new sample names

This VCF we are reheadering is about 5 Mb around the GREB1L RoSA.
```{sh}
bcftools reheader -s intermediates/004/new_names_for_vcf.txt data/greb1l-ish-region.vcf.gz > intermediates/004/fish_for_tree.vcf.gz
```


## Filter sites

We will filter sites with vcftools.  Criteria:

1. biallelic (this just drops 1000 or so out of 49,000.  Seems reasonable).
2. Genotype call in the vcf for more then 40% of the individuals.
3. Minor allele freq > 0.05

```{sh, message=FALSE}
vcftools --gzvcf intermediates/004/fish_for_tree.vcf.gz --min-alleles 2 --max-alleles 2 --max-missing 0.4 --maf 0.05 --out intermediates/004/greb1l-ish-filtered --recode 
```

## Attach ancestral alleles from coho

See `200-ancestral-states-from-coho.Rmd` for details of the alignment.  I put the resulting
fasta that is congruent with the chinook genome into `data/NC_037124.1_coho.fna` and it is
faidx indexed.

Now we attach the ancestral alleles to the vcf and filter out those that don't
have known ancestral states:
```{sh}
echo '##INFO=<ID=AA,Number=1,Type=String,Description="Ancestral allele">' > aa.hdr

bcftools +fill-from-fasta intermediates/004/greb1l-ish-filtered.recode.vcf -- -c AA -f stored_results/001/NC_037124.1_coho.fna.gz -h aa.hdr  > intermediates/004/almost-ready-for-imputation.vcf 
rm aa.hdr
```

## Extract Read Depth Information for those Genotypes 


We want to get the read depths so we
can look at that information later in the plots. So let's get it:
```{r}
rfi <- read.vcfR("intermediates/004/almost-ready-for-imputation.vcf")
rfi_depths <- vcfR2tidy(rfi)$gt %>%
  select(Indiv, POS, gt_DP) %>%
  mutate(DP = ifelse(is.na(gt_DP), 0, gt_DP)) %>%
  select(-gt_DP)
```

Now, while we are at it, let's look at the average per-site depth
for different fish in there:
```{r}
dp_means <- rfi_depths %>%
  group_by(Indiv) %>%
  summarise(mean_depth = mean(DP)) %>%
  arrange(mean_depth)
dp_means
```
And now look at that:
```{r}
ggplot(dp_means, aes(x = mean_depth)) + 
  geom_histogram(binwidth = 0.1)
```

We are going to toss anyone with average read depth less than 0.45 as that is a good
cutoff that gets all the really poor samples.  Sadly, these are mostly fish that
were sampled as carcasses in the Salmon River, but we want to keep our data clean for 
haplotyping, so we will drop those individuals at
this stage.
```{r}
dp_means %>%
  filter(mean_depth > 0.45) %>%
  .$Indiv %>%
  cat(sep = "\n", file = "intermediates/004/suff-read-depth-inds.txt")
```

Now, we grab them 
```{sh}
vcftools --vcf intermediates/004/almost-ready-for-imputation.vcf --keep intermediates/004/suff-read-depth-inds.txt --out intermediates/004/ready-for-imputation-suff-rd --recode 

cp intermediates/004/ready-for-imputation-suff-rd.recode.vcf intermediates/004/ready-for-imputation.vcf

```



# Impute and phase with BEAGLE

Others will have to adjust their paths to BEAGLE.
First the imputation
```{sh}
source script/java-jar-paths.sh

echo "Using Beagle jar at: $BEAGLE"
java -jar $BEAGLE gl=intermediates/004/ready-for-imputation.vcf  out=intermediates/004/greb1l-imputed > intermediates/004/beagle-impute-stdout.txt 

```

And then the haplotype phasing.

```{sh}
source script/java-jar-paths.sh

java -jar $BEAGLE gt=intermediates/004/greb1l-imputed.vcf.gz out=intermediates/004/greb1l-imp-phased > intermediates/004/beagle-phase-stdout.txt 
```

And after that, we reattach the ancestral states:
```{sh}
# gotta do this cuz BEAGLE doesn't put the contig name in there
tabix -f intermediates/004/greb1l-imp-phased.vcf.gz

echo '##INFO=<ID=AA,Number=1,Type=String,Description="Ancestral allele">' > aa.hdr

bcftools +fill-from-fasta intermediates/004/greb1l-imp-phased.vcf.gz -- -c AA -f stored_results/001/NC_037124.1_coho.fna.gz -h aa.hdr  > intermediates/004/greb1l-imp-phased-with-anc.vcf
rm aa.hdr
```

OK, now we have `intermediates/004/greb1l-imp-phased-with-anc.vcf` which is a VCF file of phased haplotypes
along with an ancestral allele (AA) tag in the INFO.

# Further Processing

From here, forward, we will be using those variants in R, so we will output
them in rds format for future use, along with the read-depth information.


Read in the haplotype data:
```{r}
V <- "intermediates/004/greb1l-imp-phased-with-anc.vcf"
haps <- ecaRbioinf::vcf_haplos2tidy(V, Anc = "Coho") 

# if we kept sites that don't have an ancestral allele from Coho, mark those as NA so the derived allelic states will be NA too.
haps$fix <- haps$fix %>%
  mutate(AA = ifelse(AA == "N", NA, AA))  
```

First, straighten out the haplotype names, etc.
```{r}
big_haps <- haps$tidy %>%
  filter(Indiv != "Coho") %>%
  left_join(haps$avd, by = c("ChromKey", "POS", "Indiv", "haplo")) %>% 
  mutate(haplo_name = paste0(Indiv, "-", haplo)) %>%
  left_join(rfi_depths, by = c("POS", "Indiv"))  %>% # add the read depth information in there
  mutate(pop = str_replace_all(Indiv, "(_Spring|_Late_Fall|_Fall|_Winter).*$", ""),  # add useful columns of meta data
         ecotype = str_match(Indiv, "(Spring|Late_Fall|Fall|Winter)")[, 1],
         lineage = ifelse(pop %in% c("Salmon_River", "Trinity_River_Hatchery"), "Klamath", "Sacto")
         )
```


Now, to color everything by the allele that is most common amongst springers, we recode
a column of alleles as S (for spring) and F (for fall)
```{r}
spring_ones <- big_haps %>%
  filter(ecotype == "Spring") %>%
  group_by(POS, allele) %>%
  summarise(freq = n()) %>%
  filter(rank(-freq, ties = "first") == 1) %>%
  rename(spring_allele = allele) %>%
  ungroup()

big_haps <- big_haps %>%
  left_join(spring_ones, by = "POS") %>%
  mutate(alle2 = ifelse(allele == spring_allele, "S", "F"))

```

And now we need to Join REF and ALT onto that.
Down the road, it will be important to know which allele was REF and which was ALT.
I'm going to stick that information into `big_haps2` so that we have it all, easily:
```{r}
big_haps2 <- haps$fix %>%
  select(POS, REF, ALT) %>%
  left_join(big_haps, ., by = "POS") %>%
  select(ChromKey, POS, REF, ALT, everything())
```


# Insert Variation from the Johnson Creek fish (Narum assembly)

For some of the upcoming analyses, it will be informative to include the variation seen in the
fish from Johnson Creek that Narum et al. used in their assembly.  In 200.2 I aligned their genome
to Otsh_v1.0 and made a tibble of variants.  

```{r}
# first, get the distinct positions in big_haps2
bh2d <- big_haps2 %>%
  select(ChromKey, POS, REF, ALT, spring_allele) %>%
  distinct()

# get narvars and nar_refms (Johnson Creek Variants) from 003
load("outputs/003/greb_region_narum_variants.rda")

# now, we are going to only focus on the variants called in the broader
# data set, so we will left_join stuff onto those.
all_together <- bh2d %>%
  left_join(., narvars) %>%
  left_join(., nar_refms)

# and now we create a data frame that includes all the information for
# the interior columbia individual, as if it were part of our data set.
# We do this by choosing its alleles, and naming it, etc.
jc_fish <- all_together %>%
  mutate(Indiv = "Narum_et_al_Chinook_Assembly",
         haplo = "a",
         allele = case_when(
           (!is.na(Narum_ALT) & Narum_ALT == ALT) ~ Narum_ALT,
           nchar(REF) > 1 & Narum == ALT ~ REF, # REF has no deletion, but neither did Narum.  So Narum is REF here.  A tricky one. 
           (REF == nar_refmsREF & REF == Narum) ~ Narum,
           TRUE ~ NA_character_
         ),
         haplo_name = "Narum_et_al_Chinook_Assembly-a",
         pop = "Johnson Ck.",
         ecotype = "Spring",
         lineage = "Upper Columbia",
         alle2 = ifelse(allele == spring_allele, "S", "F")) %>%
  select(-narvar_REF, -Narum_ALT, -nar_refmsREF, -Narum)
  
  
```

Note that `bind_rows(big_haps2, jc_fish)` will add the Johnson Creek fish into
the data set.

# Output final data sets as R objects

We now save the haplotypes in some tidy tibbles for future use 
in creating trees and haplo-rasters.
```{r}
write_rds(big_haps2, path = "outputs/004/big_haps2.rds", compress = "xz")
write_rds(jc_fish, path = "outputs/004/jc_fish_haps.rds", compress = "xz")
write_rds(haps$fix, path = "outputs/004/big_haps2_fixed_info.rds", compress = "xz")
```


**Note** The process of imputing and phasing with BEAGLE can produce 
different results with different runs of the algorithm.  Downstream
analyses can be subtly different, though the important patterns remain
the same.  To reproduce exactly the figures presented in the paper,
we provide the output from the BEAGLE run used for downstream 
analyses in:
```
./stored_results/004/big_haps2.rds
./stored_results/004/big_haps2_with_jc.rds
```
Downstream analyses include options (selected with a variable named like `USE_STORED_*`)
to use the stored results.



# Session Info

```{r}
sessioninfo::session_info()
```

# Running Time

Running the code and rendering this notebook required approximately this much time
on a Mac laptop of middling speed:
```{r}
Sys.time() - start_time
```

# Citations


